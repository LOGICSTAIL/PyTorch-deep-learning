{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8.1. GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HQw0kKzh3mt"
      },
      "source": [
        "Modified from:  https://towardsdatascience.com/getting-started-with-gans-using-pytorch-78e7c22a14a5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXZ4l4QHJ2Sf"
      },
      "source": [
        "### I modify the input image from 64x64x3 to 128x128x3\n",
        "The Discriminator and Generator NN to handle input image size 64x64x3 can be found in the reference above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDeNCYYzlMXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286eff9c-e5a0-4555-a9b1-1a93dbbfd8f8"
      },
      "source": [
        "import torch\n",
        "if(torch.cuda.is_available()):\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(device, torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  device= torch.device(\"cpu\")\n",
        "  print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzDSC_nh7VoZ"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLdSCpeQf8s7"
      },
      "source": [
        "latent_size=64"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYQpEktI0vGG"
      },
      "source": [
        "generator = nn.Sequential(\n",
        "    # in: latent_size x 1 x 1\n",
        "\n",
        "    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.ReLU(True),\n",
        "    # out: 512 x 4 x 4\n",
        "\n",
        "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.ReLU(True),\n",
        "    # out: 256 x 8 x 8\n",
        "\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(True),\n",
        "    # out: 128 x 16 x 16\n",
        "\n",
        "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(True),\n",
        "    # out: 64 x 32 x 32\n",
        "\n",
        "    nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(True),\n",
        "    # out: 32 x 64 x 64\n",
        "\n",
        "    nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.Tanh()\n",
        "    # out: 3 x 128 x 128\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhnfT18tlDc3",
        "outputId": "9965e8cc-e50d-4a9c-bafb-0750345737df"
      },
      "source": [
        "generator.to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (5): ReLU(inplace=True)\n",
              "  (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (8): ReLU(inplace=True)\n",
              "  (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (11): ReLU(inplace=True)\n",
              "  (12): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (14): ReLU(inplace=True)\n",
              "  (15): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (16): Tanh()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-PA-xad0zbH"
      },
      "source": [
        "discriminator = nn.Sequential(\n",
        "    # in: 3 x 128 x 128\n",
        "\n",
        "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 64 x 64 x 64\n",
        "\n",
        "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 128 x 32 x 32\n",
        "\n",
        "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 256 x 16 x 16\n",
        "\n",
        "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 512 x 8 x 8\n",
        "\n",
        "    nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(1024),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 1024 x 4 x 4\n",
        "\n",
        "    nn.Conv2d(1024, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    # out: 1 x 1 x 1\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Sigmoid())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxxP32ZTk9dg",
        "outputId": "f33ad340-dfe1-4bf1-bc1f-51b07d20dcd6"
      },
      "source": [
        "discriminator.to(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "  (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "  (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "  (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "  (12): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (13): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (14): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "  (15): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "  (16): Flatten(start_dim=1, end_dim=-1)\n",
              "  (17): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms8ZwYl8KTUd"
      },
      "source": [
        "#Connect to Google drive to generate data loader\n",
        "If you train using your own PC with Anaconda\n",
        "1. do not run drive.mount (\"/content/gdrive\", force_remount=True)\n",
        "2. train_dataset = datasets.ImageFolder(root = \"C:/Users/ADMIN/Google 雲端硬碟/Image folders/train\", transform = transformer) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi5yHsIRKTU-",
        "outputId": "bfa5134f-5f2b-4602-a2d5-4765a88612b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf2d_1toKqsD"
      },
      "source": [
        "image_size = 128\n",
        "batch_size = 64"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXe0_ovEKm0q"
      },
      "source": [
        "from torchvision import transforms\n",
        "transformer = transforms.Compose([\n",
        "  transforms.Resize((image_size, image_size)),       \n",
        "  transforms.ToTensor(),                     \n",
        "  transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5] )])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnihKRVmKTVC"
      },
      "source": [
        "from torchvision import datasets\n",
        "train_dataset = datasets.ImageFolder(root = \"/content/gdrive/MyDrive/Image folders/train\", transform = transformer)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhHOwRz3KTVD"
      },
      "source": [
        "import torch.utils.data as Data\n",
        "train_dl = Data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n06IVvaloTEl"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D54wmcOl87Fx"
      },
      "source": [
        "def train_discriminator(real_images, opt_d):\n",
        "  # Clear discriminator gradients\n",
        "  opt_d.zero_grad()\n",
        "\n",
        "  # Pass real images through discriminator\n",
        "  real_preds = discriminator(real_images)\n",
        "  real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
        "  real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
        "  real_score = torch.mean(real_preds).item()\n",
        "  \n",
        "  # Generate fake images\n",
        "  latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
        "  fake_images = generator(latent.to(device))\n",
        "\n",
        "  # Pass fake images through discriminator\n",
        "  fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
        "  fake_preds = discriminator(fake_images)\n",
        "  fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
        "  fake_score = torch.mean(fake_preds).item()\n",
        "\n",
        "  # Update discriminator weights\n",
        "  loss = real_loss + fake_loss\n",
        "  loss.backward()\n",
        "  opt_d.step()\n",
        "  return loss.item(), real_score, fake_score"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrrgPeEi07sb"
      },
      "source": [
        "def train_generator(opt_g):\n",
        "  # Clear generator gradients\n",
        "  opt_g.zero_grad()\n",
        "  \n",
        "  # Generate fake images\n",
        "  latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
        "  fake_images = generator(latent)\n",
        "  \n",
        "  # Try to fool the discriminator\n",
        "  preds = discriminator(fake_images)\n",
        "  targets = torch.ones(batch_size, 1, device=device)\n",
        "  loss = F.binary_cross_entropy(preds, targets)\n",
        "  \n",
        "  # Update generator weights\n",
        "  loss.backward()\n",
        "  opt_g.step()\n",
        "  \n",
        "  return loss.item()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc_GmVXLK6wI"
      },
      "source": [
        "# Image saving untility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjR3kuPapcLs"
      },
      "source": [
        "from torchvision.utils import save_image"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDcp9VL-olBq"
      },
      "source": [
        "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z_ywnhFK4OE"
      },
      "source": [
        "def denorm(img_tensors):\n",
        "  return img_tensors * stats[1][0] + stats[0][0]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPvrJdxxK4OG"
      },
      "source": [
        "import os\n",
        "sample_dir = 'generated'\n",
        "os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "def save_samples(index, latent_tensors, show=True):\n",
        "  fake_images = generator(latent_tensors)\n",
        "  fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
        "  save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
        "  print('Saving', fake_fname)\n",
        "  if show:\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14xm6dnAmGvd"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoBnqeNJ2bhs"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOfDVT1To1EU"
      },
      "source": [
        "# Losses & scores\n",
        "losses_g = []\n",
        "losses_d = []\n",
        "real_scores = []\n",
        "fake_scores = []"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crH70nd1hHd7"
      },
      "source": [
        "lr = 0.0002"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bnb2FP62hcx"
      },
      "source": [
        "# Create optimizers\n",
        "opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcjrZR2s3_Vj"
      },
      "source": [
        "epochs =1200\n",
        "start_idx=1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpNSL9pZsxSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ecbce56-8f68-4d46-d7c0-34609957f11b"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  if(epoch % 10 ==0):\n",
        "    print(epoch, end=\",\")\n",
        "  for real_images, _ in train_dl:\n",
        "    # Train discriminator\n",
        "    loss_d, real_score, fake_score = train_discriminator(real_images.to(device), opt_d)\n",
        "    # Train generator\n",
        "    loss_g = train_generator(opt_g)\n",
        "      \n",
        "  # Record losses & scores\n",
        "  losses_g.append(loss_g)\n",
        "  losses_d.append(loss_d)\n",
        "  real_scores.append(real_score)\n",
        "  fake_scores.append(fake_score)\n",
        "  \n",
        "  if(epoch % 50 ==0):\n",
        "    # Log losses & scores (last batch)\n",
        "    print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
        "        epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
        "    # Save generated images\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
        "    save_samples(epoch+start_idx, latent, show=False)\n",
        "    #save_samples(epoch+start_idx, fixed_latent, show=False)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0,Epoch [1/1200], loss_g: 5.6803, loss_d: 0.4101, real_score: 0.8384, fake_score: 0.1615\n",
            "Saving generated-images-0001.png\n",
            "10,20,30,40,50,Epoch [51/1200], loss_g: 7.5752, loss_d: 0.0027, real_score: 0.9996, fake_score: 0.0023\n",
            "Saving generated-images-0051.png\n",
            "60,70,80,90,100,Epoch [101/1200], loss_g: 8.4624, loss_d: 0.0007, real_score: 0.9997, fake_score: 0.0004\n",
            "Saving generated-images-0101.png\n",
            "110,120,130,140,150,Epoch [151/1200], loss_g: 6.0435, loss_d: 0.0338, real_score: 0.9750, fake_score: 0.0050\n",
            "Saving generated-images-0151.png\n",
            "160,170,180,190,200,Epoch [201/1200], loss_g: 7.4477, loss_d: 0.0020, real_score: 0.9996, fake_score: 0.0016\n",
            "Saving generated-images-0201.png\n",
            "210,220,230,240,250,Epoch [251/1200], loss_g: 1.6239, loss_d: 0.0363, real_score: 0.9678, fake_score: 0.0008\n",
            "Saving generated-images-0251.png\n",
            "260,270,280,290,300,Epoch [301/1200], loss_g: 4.5490, loss_d: 0.7125, real_score: 0.8574, fake_score: 0.1723\n",
            "Saving generated-images-0301.png\n",
            "310,320,330,340,350,Epoch [351/1200], loss_g: 2.3434, loss_d: 0.6248, real_score: 0.6541, fake_score: 0.0680\n",
            "Saving generated-images-0351.png\n",
            "360,370,380,390,400,Epoch [401/1200], loss_g: 2.6788, loss_d: 0.4491, real_score: 0.7114, fake_score: 0.0042\n",
            "Saving generated-images-0401.png\n",
            "410,420,430,440,450,Epoch [451/1200], loss_g: 4.9476, loss_d: 0.4670, real_score: 0.9137, fake_score: 0.0481\n",
            "Saving generated-images-0451.png\n",
            "460,470,480,490,500,Epoch [501/1200], loss_g: 6.6407, loss_d: 0.2239, real_score: 0.9714, fake_score: 0.1524\n",
            "Saving generated-images-0501.png\n",
            "510,520,530,540,550,Epoch [551/1200], loss_g: 4.1673, loss_d: 0.2900, real_score: 0.8110, fake_score: 0.0389\n",
            "Saving generated-images-0551.png\n",
            "560,"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBYi_a4bpIaJ"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}